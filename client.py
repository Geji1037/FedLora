

import ray
import torch
import socket
from typing import Dict
import os

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training,set_peft_model_state_dict
from train_utils import train_one_step

from torch.utils.data import DataLoader
from transformers import default_data_collator

@ray.remote
class Client:
    def __init__(self, cid: int,model_path:str):
        self.cid = cid
        cvd = os.environ.get("CUDA_VISIBLE_DEVICES")
        has_cuda = torch.cuda.is_available()
        ndev = torch.cuda.device_count()   
        self.device = torch.device("cuda:0" if has_cuda and ndev > 0 else "cpu")
        self.hostname = socket.gethostname()
        self.model_path = model_path
        print(f"[Client {self.cid}] host={self.hostname} "
              f"CVD={cvd} cuda_avail={has_cuda} ndev={ndev} -> device={self.device}")
        if has_cuda and ndev > 0:
            try:
                print(f"[Client {self.cid}] GPU_NAME={torch.cuda.get_device_name(0)}")
            except Exception as e:
                print(f"[Client {self.cid}] GPU_NAME_ERR={e}")
        self.model = None
        self.tokenizer = None

    def load_model(self,param_template:Dict[str,torch.Tensor]):
        print(f"[Client {self.cid}] åˆå§‹åŒ–æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        base_model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            local_files_only=True
        )

        # ç”¨ LoRA åŒ…è£…æ¨¡å‹ï¼ˆæ³¨æ„è¦é…ç½®æ­£ç¡®çš„ target modulesï¼‰
        lora_config = LoraConfig(
            r=8,
            lora_alpha=32,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        model = get_peft_model(base_model, lora_config)
        model = model.to(self.device)
        model.train()
        self.model = model

        if param_template and any("lora_" in k for k in param_template.keys()):
            # å°†èšåˆæ¥çš„ LoRA å†™å…¥æœ¬åœ° LoRA
            lora_loaded_missing, lora_unexpected = set_peft_model_state_dict(
                self.model,
                {k: v.to(self.device, dtype=self.model.dtype) for k, v in param_template.items()},
                adapter_name="default"
            )
            matched = len(param_template) - len(lora_loaded_missing)
            print(f"[Client {self.cid}] åŠ è½½èšåˆLoRAï¼šåŒ¹é…{matched} ç¼ºå¤±{len(lora_loaded_missing)} æ„å¤–{len(lora_unexpected)}")
        else:
            # å…¼å®¹æ—§è·¯å¾„ï¼ˆå¦‚æœè¿˜æ˜¯å‘æ¥ q_proj/v_proj çš„å…¨é‡çŸ©é˜µï¼Œå°±æŒ‰æ—§é€»è¾‘è¦†ç›–ï¼‰
            loaded, missing = 0, []
            for name, param in model.named_parameters():
                if name in param_template:
                    param.data = param_template[name].to(self.device).to(param.dtype)
                    loaded += 1
                elif "lora_" in name:
                    missing.append(name)
            print(f"[Client {self.cid}] è¦†ç›–å‚æ•°æ•°ï¼š{loaded}ï¼›æœªåŒ¹é…çš„ LoRA å‚æ•°æ ·ä¾‹ï¼š{missing[:5]}")


        # æ‰“å°ä¸€ä¸‹ç»“æ„
        # print(model)

        # missing ,loaded = [],0

        # # å°†æ¥è‡ª Aggregator çš„æƒé‡åŠ è½½åˆ° model ä¸­
        # for name, param in model.named_parameters():
        #     if name in param_template:
        #         param.data = param_template[name].to(self.device).to(param.dtype)
        #         loaded += 1
        #     elif "lora_" in name:
        #         missing.append(name)
        # print(f"[Client {self.cid}] è¦†ç›–å‚æ•°æ•°ï¼š{loaded}ï¼›æœªåŒ¹é…çš„ LoRA å‚æ•°æ ·ä¾‹ï¼š{missing[:5]}")

    def ping(self):
        return {"cid": self.cid, "host": self.hostname, "device": str(self.device)}
        pass

    def process_parameters(self, params: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """æ¥æ”¶å‚æ•°ï¼ŒåŠ è½½æ¨¡å‹ï¼Œæ‰§è¡Œæœ¬åœ°è®­ç»ƒï¼Œå¹¶è¿”å›æ›´æ–°åçš„æƒé‡"""
        self.load_model(params)

        # ğŸ‘‰ æ‰§è¡Œä¸€è½®è®­ç»ƒï¼ˆå¯ä»¥æ”¾åˆ° train_utils.py ä¸­ï¼‰
        train_one_step(self.model, self.tokenizer, self.device)

        # è¿”å›æ›´æ–°åçš„ LoRA æƒé‡
        updated_params = {
            n: p.detach().cpu().half()
            for n, p in self.model.state_dict().items()
            if "lora_" in n and n.endswith("weight")
        }

        return {"lora_state": updated_params, "num_samples": 1024}
        # updated_params = {
        #     n: p.detach().cpu().half() for n, p in self.model.named_parameters()
        #     if "lora_" in n  # åªå‘é€ LoRA æƒé‡
        # }

        # return updated_params
