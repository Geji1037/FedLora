

import ray
import torch
import socket
from typing import Dict

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training
from train_utils import train_one_step

from torch.utils.data import DataLoader
from transformers import default_data_collator

@ray.remote
class Client:
    def __init__(self, cid: int,model_path:str):
        self.cid = cid
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.hostname = socket.gethostname()
        self.model_path = model_path
        print(f"[Client {self.cid}] åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡ï¼š{self.device}, ä¸»æœºåï¼š{self.hostname}")
        self.model = None
        self.tokenizer = None

    def load_model(self,param_template:Dict[str,torch.Tensor]):
        print(f"[Client {self.cid}] åˆå§‹åŒ–æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        base_model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            device_map={"": self.device},
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            local_files_only=True
        )

        # ç”¨ LoRA åŒ…è£…æ¨¡å‹ï¼ˆæ³¨æ„è¦é…ç½®æ­£ç¡®çš„ target modulesï¼‰
        lora_config = LoraConfig(
            r=8,
            lora_alpha=32,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        model = get_peft_model(base_model, lora_config)
        model = model.to(self.device)
        model.train()
        self.model = model

        # æ‰“å°ä¸€ä¸‹ç»“æ„
        print(model)

        # å°†æ¥è‡ª Aggregator çš„æƒé‡åŠ è½½åˆ° model ä¸­
        for name, param in model.named_parameters():
            if name in param_template:
                param.data = param_template[name].to(self.device).to(param.dtype)

    def ping(self):
        pass

    def process_parameters(self, params: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """æ¥æ”¶å‚æ•°ï¼ŒåŠ è½½æ¨¡å‹ï¼Œæ‰§è¡Œæœ¬åœ°è®­ç»ƒï¼Œå¹¶è¿”å›æ›´æ–°åçš„æƒé‡"""
        self.load_model(params)

        # ğŸ‘‰ æ‰§è¡Œä¸€è½®è®­ç»ƒï¼ˆå¯ä»¥æ”¾åˆ° train_utils.py ä¸­ï¼‰
        train_one_step(self.model, self.tokenizer, self.device)

        # è¿”å›æ›´æ–°åçš„ LoRA æƒé‡
        updated_params = {
            n: p.detach().cpu().half() for n, p in self.model.named_parameters()
            if "lora_" in n  # åªå‘é€ LoRA æƒé‡
        }

        return updated_params
